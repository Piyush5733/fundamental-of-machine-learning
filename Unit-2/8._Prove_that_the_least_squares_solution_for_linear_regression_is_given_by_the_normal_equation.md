
# Prove that the least squares solution for linear regression is given by:  `θ = (X^T * X)^(-1) * X^T * y`

**Answer:**

This equation is known as the **Normal Equation** for linear regression. It provides a closed-form solution for the optimal parameters (θ) that minimize the sum of squared errors. Here is a step-by-step proof:

### 1. Define the Linear Regression Model

Let's assume we have a dataset with `m` training examples and `n` features.

*   **X** is the `m x (n+1)` matrix of input features, where the first column is all ones to account for the intercept term.
*   **y** is the `m x 1` vector of target values.
*   **θ** is the `(n+1) x 1` vector of parameters (weights) that we want to find.

The linear regression model can be written in matrix form as:

**h(X) = Xθ**

where `h(X)` is the vector of predicted values.

### 2. Define the Cost Function (Sum of Squared Errors)

The cost function, `J(θ)`, for linear regression is the sum of the squared differences between the predicted values and the actual values. In matrix form, this can be written as:

**J(θ) = (1/2m) * Σ(h(xi) - yi)²**

For simplicity in derivation, we can ignore the `1/2m` constant, as it doesn't affect the value of θ that minimizes the function. So, we'll work with:

**J(θ) = Σ(h(xi) - yi)²**

In matrix form, this is:

**J(θ) = (Xθ - y)ᵀ(Xθ - y)**

### 3. Expand the Cost Function

Now, let's expand the cost function using the rules of matrix transpose: `(AB)ᵀ = BᵀAᵀ`.

**J(θ) = ( (Xθ)ᵀ - yᵀ ) (Xθ - y)**
**J(θ) = ( θᵀXᵀ - yᵀ ) (Xθ - y)**
**J(θ) = θᵀXᵀXθ - θᵀXᵀy - yᵀXθ + yᵀy**

Since `yᵀXθ` is a scalar, its transpose is equal to itself: `(yᵀXθ)ᵀ = θᵀXᵀy`. So, we can combine the two middle terms:

**J(θ) = θᵀXᵀXθ - 2θᵀXᵀy + yᵀy**

### 4. Take the Derivative of the Cost Function

To find the value of θ that minimizes the cost function, we need to take the derivative of `J(θ)` with respect to `θ` and set it to zero. We will use the following matrix calculus rules:

*   **d(AᵀB)/dB = A**
*   **d(BᵀAB)/dB = 2AB** (if A is symmetric)

In our cost function, `XᵀX` is a symmetric matrix. So, we can apply these rules:

**∇_θ J(θ) = ∇_θ (θᵀXᵀXθ - 2θᵀXᵀy + yᵀy)**
**∇_θ J(θ) = ∇_θ (θᵀ(XᵀX)θ) - ∇_θ (2θᵀ(Xᵀy)) + ∇_θ (yᵀy)**

Applying the rules:

*   The derivative of the first term is `2(XᵀX)θ`.
*   The derivative of the second term is `2Xᵀy`.
*   The derivative of the third term (which doesn't contain θ) is `0`.

So, the derivative of the cost function is:

**∇_θ J(θ) = 2(XᵀX)θ - 2Xᵀy**

### 5. Set the Derivative to Zero

Now, we set the derivative to zero to find the minimum:

**2(XᵀX)θ - 2Xᵀy = 0**
**(XᵀX)θ - Xᵀy = 0**
**(XᵀX)θ = Xᵀy**

### 6. Solve for θ

Finally, we solve for θ by multiplying both sides by the inverse of `(XᵀX)`:

**θ = (XᵀX)⁻¹Xᵀy**

This is the Normal Equation. It gives us the value of θ that minimizes the cost function, and thus provides the least squares solution for linear regression.

**Note:** The matrix `(XᵀX)` must be invertible for this solution to exist. If it is not invertible (i.e., it is singular), it may be because of redundant features (multicollinearity) or having more features than training examples. In such cases, other methods like regularization (e.g., Ridge Regression) or gradient descent are used.
